#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡ç« é“¾æ¥è½¬å‘åˆ°ä»Šæ—¥å¤´æ¡å·¥å…· v2.0
æ”¯æŒä»å„ç§ç½‘ç«™æŠ“å–æ–‡ç« å†…å®¹å¹¶è½¬å‘åˆ°ä»Šæ—¥å¤´æ¡
ä¼˜åŒ–æ’ç‰ˆï¼Œæå‡é˜…è¯»ä½“éªŒ
"""

import asyncio
import os
import sys
import re
import time
import hashlib
from datetime import datetime
from urllib.parse import urlparse
from playwright.async_api import async_playwright
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
import markdown

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from uploader.toutiao_uploader.main import TouTiaoArticle, toutiao_setup

class EnhancedArticleForwarder:
    """å¢å¼ºç‰ˆæ–‡ç« è½¬å‘å·¥å…·"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.site_configs = {
            'juejin.cn': self._extract_juejin,
            'cnblogs.com': self._extract_cnblogs,
            'csdn.net': self._extract_csdn,
            'jianshu.com': self._extract_jianshu,
            'zhihu.com': self._extract_zhihu,
            'segmentfault.com': self._extract_segmentfault,
            'oschina.net': self._extract_oschina,
        }
        
        # å†…å®¹ç¾åŒ–é…ç½®
        self.content_enhancers = {
            'code_languages': {
                'javascript': 'JavaScript', 'js': 'JavaScript',
                'typescript': 'TypeScript', 'ts': 'TypeScript',
                'python': 'Python', 'py': 'Python',
                'java': 'Java', 'cpp': 'C++', 'c': 'C',
                'html': 'HTML', 'css': 'CSS', 'scss': 'SCSS',
                'shell': 'Shell', 'bash': 'Bash', 'sql': 'SQL',
                'json': 'JSON', 'xml': 'XML', 'yaml': 'YAML'
            },
            'emoji_mapping': {
                'å‰è¨€': 'ğŸ“', 'ä»‹ç»': 'ğŸ“–', 'æ¦‚è¿°': 'ğŸ”',
                'å®‰è£…': 'âš™ï¸', 'é…ç½®': 'ğŸ”§', 'ä½¿ç”¨': 'ğŸš€',
                'ç¤ºä¾‹': 'ğŸ’¡', 'ä¾‹å­': 'ğŸ’¡', 'ä»£ç ': 'ğŸ’»',
                'æ€»ç»“': 'ğŸ“‹', 'ç»“è®º': 'ğŸ¯', 'å°ç»“': 'ğŸ“',
                'æ³¨æ„': 'âš ï¸', 'è­¦å‘Š': 'ğŸš¨', 'æç¤º': 'ğŸ’¡',
                'ä¼˜ç‚¹': 'âœ…', 'ç¼ºç‚¹': 'âŒ', 'ç‰¹ç‚¹': 'ğŸ¯',
                'æ­¥éª¤': 'ğŸ“', 'æ–¹æ³•': 'ğŸ”§', 'æŠ€å·§': 'ğŸ’¡',
                'é—®é¢˜': 'â“', 'è§£å†³': 'âœ…', 'é”™è¯¯': 'âŒ',
                'æ€§èƒ½': 'âš¡', 'å®‰å…¨': 'ğŸ”’', 'æµ‹è¯•': 'ğŸ§ª'
            }
        }
        
        # åˆå§‹åŒ–Markdownè½¬æ¢å™¨
        self.markdown_converter = markdown.Markdown(
            extensions=[
                'markdown.extensions.extra',
                'markdown.extensions.codehilite',
                'markdown.extensions.tables',
                'markdown.extensions.toc',
                'markdown.extensions.fenced_code',
                'markdown.extensions.nl2br'
            ],
            extension_configs={
                'codehilite': {
                    'css_class': 'highlight',
                    'use_pygments': False
                }
            }
        )
    
    def _markdown_to_html(self, markdown_content):
        """å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML"""
        if not markdown_content:
            return ""
        
        try:
            # é‡ç½®è½¬æ¢å™¨çŠ¶æ€
            self.markdown_converter.reset()
            
            # è½¬æ¢Markdownä¸ºHTML
            html_content = self.markdown_converter.convert(markdown_content)
            
            # ä¼˜åŒ–HTMLæ ¼å¼
            html_content = self._optimize_html_format(html_content)
            
            return html_content
            
        except Exception as e:
            print(f"âš ï¸ Markdownè½¬æ¢å¤±è´¥: {e}")
            return markdown_content
    
    def _optimize_html_format(self, html_content):
        """ä¼˜åŒ–HTMLæ ¼å¼"""
        if not html_content:
            return ""
        
        # ä¸ºæ®µè½æ·»åŠ é—´è·
        html_content = re.sub(r'<p>', '<p style="margin: 16px 0; line-height: 1.6;">', html_content)
        
        # ä¸ºæ ‡é¢˜æ·»åŠ æ ·å¼
        html_content = re.sub(r'<h1>', '<h1 style="font-size: 24px; font-weight: bold; margin: 24px 0 16px 0; color: #333;">', html_content)
        html_content = re.sub(r'<h2>', '<h2 style="font-size: 22px; font-weight: bold; margin: 20px 0 14px 0; color: #333; border-bottom: 2px solid #eee; padding-bottom: 8px;">', html_content)
        html_content = re.sub(r'<h3>', '<h3 style="font-size: 20px; font-weight: bold; margin: 18px 0 12px 0; color: #333;">', html_content)
        html_content = re.sub(r'<h4>', '<h4 style="font-size: 18px; font-weight: bold; margin: 16px 0 10px 0; color: #333;">', html_content)
        html_content = re.sub(r'<h5>', '<h5 style="font-size: 16px; font-weight: bold; margin: 14px 0 8px 0; color: #333;">', html_content)
        html_content = re.sub(r'<h6>', '<h6 style="font-size: 14px; font-weight: bold; margin: 12px 0 6px 0; color: #333;">', html_content)
        
        # ä¸ºé“¾æ¥æ·»åŠ æ ·å¼
        html_content = re.sub(r'<a ', '<a style="color: #1890ff; text-decoration: none;" ', html_content)
        
        # ä¸ºå¼ºè°ƒæ–‡æœ¬æ·»åŠ æ ·å¼
        html_content = re.sub(r'<strong>', '<strong style="font-weight: bold; color: #333;">', html_content)
        html_content = re.sub(r'<em>', '<em style="font-style: italic; color: #666;">', html_content)
        
        # ä¸ºä»£ç æ·»åŠ æ ·å¼
        html_content = re.sub(
            r'<code>',
            '<code style="background-color: #f6f8fa; color: #d73a49; padding: 2px 4px; border-radius: 3px; font-family: monospace; font-size: 0.9em;">',
            html_content
        )
        
        # ä¸ºä»£ç å—æ·»åŠ æ ·å¼
        html_content = re.sub(
            r'<pre>',
            '<pre style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 16px; overflow-x: auto; margin: 16px 0; font-family: monospace; font-size: 14px; line-height: 1.45;">',
            html_content
        )
        
        # ä¸ºå¼•ç”¨å—æ·»åŠ æ ·å¼
        html_content = re.sub(
            r'<blockquote>',
            '<blockquote style="border-left: 4px solid #dfe2e5; margin: 16px 0; padding: 0 16px; color: #6a737d; background-color: #f8f9fa; border-radius: 0 6px 6px 0;">',
            html_content
        )
        
        # ä¸ºåˆ—è¡¨æ·»åŠ æ ·å¼
        html_content = re.sub(r'<ul>', '<ul style="margin: 16px 0; padding-left: 24px;">', html_content)
        html_content = re.sub(r'<ol>', '<ol style="margin: 16px 0; padding-left: 24px;">', html_content)
        html_content = re.sub(r'<li>', '<li style="margin: 4px 0; line-height: 1.6;">', html_content)
        
        # ä¸ºè¡¨æ ¼æ·»åŠ æ ·å¼
        html_content = re.sub(r'<table>', '<table style="border-collapse: collapse; width: 100%; border: 1px solid #e1e4e8; margin: 16px 0;">', html_content)
        html_content = re.sub(r'<th>', '<th style="background-color: #f6f8fa; border: 1px solid #e1e4e8; padding: 8px 12px; text-align: left; font-weight: bold;">', html_content)
        html_content = re.sub(r'<td>', '<td style="border: 1px solid #e1e4e8; padding: 8px 12px;">', html_content)
        
        return html_content
    
    def _markdown_to_rich_text(self, markdown_content):
        """å°†Markdownå†…å®¹è½¬æ¢ä¸ºå¯Œæ–‡æœ¬æ ¼å¼ï¼ˆä¸æ˜¯HTMLä»£ç ï¼‰"""
        if not markdown_content:
            return ""
        
        try:
            # å…ˆè½¬æ¢ä¸ºHTML
            html_content = self.markdown_converter.convert(markdown_content)
            
            # å°†HTMLè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼Œä½†ä¿ç•™æ ¼å¼æ•ˆæœ
            rich_text = self._html_to_formatted_text(html_content)
            
            return rich_text
            
        except Exception as e:
            print(f"âš ï¸ å¯Œæ–‡æœ¬è½¬æ¢å¤±è´¥: {e}")
            return self._markdown_to_plain_text(markdown_content)
    
    def _html_to_formatted_text(self, html_content):
        """å°†HTMLè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„çº¯æ–‡æœ¬"""
        if not html_content:
            return ""
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # é€’å½’å¤„ç†HTMLå…ƒç´ ï¼Œè½¬æ¢ä¸ºæ ¼å¼åŒ–æ–‡æœ¬
        def process_html_element(element, depth=0):
            if isinstance(element, NavigableString):
                return str(element).strip()
            
            if not hasattr(element, 'name'):
                return ""
            
            tag_name = element.name.lower()
            
            # è·å–å­å…ƒç´ å†…å®¹
            children_text = []
            for child in element.children:
                child_text = process_html_element(child, depth + 1)
                if child_text:
                    children_text.append(child_text)
            
            content = ' '.join(children_text) if children_text else element.get_text().strip()
            
            # æ ¹æ®HTMLæ ‡ç­¾è½¬æ¢ä¸ºç›¸åº”çš„æ–‡æœ¬æ ¼å¼
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                if content:
                    # ä½¿ç”¨ä¸åŒçš„è£…é¥°ç¬¦è¡¨ç¤ºæ ‡é¢˜å±‚çº§
                    if level == 1:
                        return f"\n\n{'=' * 50}\n{content}\n{'=' * 50}\n\n"
                    elif level == 2:
                        return f"\n\n{'-' * 40}\n{content}\n{'-' * 40}\n\n"
                    elif level == 3:
                        return f"\n\nâ–¶ {content}\n{'-' * len(content)}\n\n"
                    else:
                        return f"\n\nâ— {content}\n\n"
            
            elif tag_name == 'p':
                if content:
                    return f"\n\n{content}\n\n"
            
            elif tag_name in ['strong', 'b']:
                if content:
                    return f"ã€{content}ã€‘"  # ä½¿ç”¨ä¸­æ–‡æ‹¬å·è¡¨ç¤ºç²—ä½“
            
            elif tag_name in ['em', 'i']:
                if content:
                    return f"ã€Š{content}ã€‹"  # ä½¿ç”¨ä¹¦åå·è¡¨ç¤ºæ–œä½“
            
            elif tag_name == 'code':
                if content:
                    return f"ã€Œ{content}ã€"  # ä½¿ç”¨ç›´è§’å¼•å·è¡¨ç¤ºä»£ç 
            
            elif tag_name == 'pre':
                if content:
                    return f"\n\nâ”Œâ”€ ä»£ç ç¤ºä¾‹ â”€â”\n{content}\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n"
            
            elif tag_name in ['ul', 'ol']:
                if children_text:
                    return f"\n\n" + "\n".join(children_text) + "\n\n"
            
            elif tag_name == 'li':
                if content:
                    return f"â€¢ {content}"
            
            elif tag_name == 'blockquote':
                if content:
                    lines = content.split('\n')
                    quoted_lines = [f"â”ƒ {line.strip()}" for line in lines if line.strip()]
                    return f"\n\n" + "\n".join(quoted_lines) + "\n\n"
            
            elif tag_name == 'a':
                href = element.get('href', '')
                if content and href:
                    return f"{content}ï¼ˆ{href}ï¼‰"
                else:
                    return content
            
            elif tag_name == 'table':
                return f"\n\nğŸ“Š è¡¨æ ¼æ•°æ®ï¼š\n{content}\n\n"
            
            elif tag_name in ['th', 'td']:
                return f" {content} |"
            
            elif tag_name == 'tr':
                return f"|{content}\n"
            
            elif tag_name == 'hr':
                return f"\n\n{'â”€' * 50}\n\n"
            
            elif tag_name == 'br':
                return "\n"
            
            else:
                return content
        
        # å¤„ç†æ•´ä¸ªHTMLæ–‡æ¡£
        result = process_html_element(soup)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        if result:
            result = re.sub(r'\n{3,}', '\n\n', result)
            result = result.strip()
        
        return result
    
    def _markdown_to_plain_text(self, markdown_content):
        """å°†Markdownè½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if not markdown_content:
            return ""
        
        # ç®€å•çš„Markdownåˆ°æ–‡æœ¬è½¬æ¢
        text = markdown_content
        
        # å¤„ç†æ ‡é¢˜
        text = re.sub(r'^#{1}\s+(.+)$', r'\n\n=== \1 ===\n\n', text, flags=re.MULTILINE)
        text = re.sub(r'^#{2}\s+(.+)$', r'\n\n--- \1 ---\n\n', text, flags=re.MULTILINE)
        text = re.sub(r'^#{3,6}\s+(.+)$', r'\n\nâ–¶ \1\n\n', text, flags=re.MULTILINE)
        
        # å¤„ç†ç²—ä½“å’Œæ–œä½“
        text = re.sub(r'\*\*(.+?)\*\*', r'ã€\1ã€‘', text)
        text = re.sub(r'\*(.+?)\*', r'ã€Š\1ã€‹', text)
        
        # å¤„ç†ä»£ç 
        text = re.sub(r'`(.+?)`', r'ã€Œ\1ã€', text)
        text = re.sub(r'```[\w]*\n(.*?)\n```', r'\n\nâ”Œâ”€ ä»£ç ç¤ºä¾‹ â”€â”\n\1\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n', text, flags=re.DOTALL)
        
        # å¤„ç†å¼•ç”¨
        text = re.sub(r'^>\s*(.+)$', r'â”ƒ \1', text, flags=re.MULTILINE)
        
        # å¤„ç†åˆ—è¡¨
        text = re.sub(r'^[-*+]\s+(.+)$', r'â€¢ \1', text, flags=re.MULTILINE)
        text = re.sub(r'^\d+\.\s+(.+)$', r'â€¢ \1', text, flags=re.MULTILINE)
        
        # å¤„ç†é“¾æ¥
        text = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'\1ï¼ˆ\2ï¼‰', text)
        
        # å¤„ç†åˆ†éš”çº¿
        text = re.sub(r'^---+$', 'â”€' * 50, text, flags=re.MULTILINE)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # å¤šä¸ªæ¢è¡Œå˜ä¸ºä¸¤ä¸ª
        text = re.sub(r'[ \t]+', ' ', text)  # å¤šä¸ªç©ºæ ¼å˜ä¸ºä¸€ä¸ª
        text = re.sub(r'^\s+|\s+$', '', text, flags=re.MULTILINE)  # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)  # é›¶å®½å­—ç¬¦
        text = text.strip()
        
        return text
    
    def _remove_unwanted_elements(self, soup):
        """ç§»é™¤ä¸éœ€è¦çš„HTMLå…ƒç´ """
        # ç§»é™¤scriptã€styleã€navç­‰ä¸éœ€è¦çš„æ ‡ç­¾
        unwanted_tags = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            'advertisement', 'ad', 'sidebar', 'menu', 'breadcrumb',
            'noscript', 'iframe', 'embed', 'object'
        ]
        
        for tag_name in unwanted_tags:
            for tag in soup.find_all(tag_name):
                tag.decompose()
        
        # ç§»é™¤å¸¸è§çš„å¹¿å‘Šå’Œæ— å…³å†…å®¹çš„class/id
        unwanted_patterns = [
            r'ad[_-]', r'advertisement', r'sponsor', r'promo', 
            r'sidebar', r'related', r'comment', r'footer', 
            r'header', r'nav', r'breadcrumb', r'pagination', 
            r'share', r'social', r'tracking', r'analytics'
        ]
        
        for pattern in unwanted_patterns:
            for tag in soup.find_all(class_=re.compile(pattern, re.I)):
                tag.decompose()
            for tag in soup.find_all(id=re.compile(pattern, re.I)):
                tag.decompose()
        
        return soup
    
    def _enhance_title_with_emoji(self, title):
        """ä¸ºæ ‡é¢˜æ·»åŠ åˆé€‚çš„emoji"""
        title_lower = title.lower()
        
        for keyword, emoji in self.content_enhancers['emoji_mapping'].items():
            if keyword in title_lower:
                if not title.startswith(emoji):
                    return f"{emoji} {title}"
                break
        
        return title
    
    def _detect_code_language(self, code_elem):
        """æ£€æµ‹ä»£ç è¯­è¨€"""
        # ä»classå±æ€§æ£€æµ‹
        class_attr = code_elem.get('class', [])
        if class_attr:
            for cls in class_attr:
                for lang_key, lang_name in self.content_enhancers['code_languages'].items():
                    if lang_key in cls.lower():
                        return lang_key
        
        # ä»å†…å®¹ç‰¹å¾æ£€æµ‹
        code_text = code_elem.get_text()[:200].lower()
        
        language_patterns = {
            'javascript': [r'function\s*\(', r'const\s+\w+', r'let\s+\w+', r'console\.log'],
            'python': [r'def\s+\w+\(', r'import\s+\w+', r'from\s+\w+\s+import', r'print\('],
            'java': [r'public\s+class', r'public\s+static\s+void', r'System\.out\.println'],
            'css': [r'\.\w+\s*{', r'#\w+\s*{', r'@media', r'flex'],
            'html': [r'<html', r'<div', r'<span', r'<!DOCTYPE'],
            'json': [r'^\s*[{\[]', r'"\w+":', r'},\s*"'],
            'sql': [r'SELECT\s+', r'FROM\s+', r'WHERE\s+', r'INSERT\s+INTO']
        }
        
        for lang, patterns in language_patterns.items():
            if any(re.search(pattern, code_text, re.I) for pattern in patterns):
                return lang
        
        return ''
    
    def _format_code_block(self, code_elem):
        """æ ¼å¼åŒ–ä»£ç å—"""
        code_text = code_elem.get_text()
        
        # æ£€æµ‹è¯­è¨€
        language = self._detect_code_language(code_elem)
        lang_display = self.content_enhancers['code_languages'].get(language, language)
        
        # æ¸…ç†ä»£ç æ–‡æœ¬
        code_text = code_text.strip()
        
        # æ·»åŠ è¯­è¨€æ ‡æ³¨
        if lang_display:
            return f"\n\nğŸ’» **{lang_display} ä»£ç ç¤ºä¾‹ï¼š**\n\n```{language}\n{code_text}\n```\n\n"
        else:
            return f"\n\n```\n{code_text}\n```\n\n"
    
    def _format_list_item(self, li_elem, list_type='ul', index=1):
        """æ ¼å¼åŒ–åˆ—è¡¨é¡¹"""
        text = li_elem.get_text().strip()
        
        if list_type == 'ol':
            prefix = f"{index}."
        else:
            # æ ¹æ®å†…å®¹é€‰æ‹©ä¸åŒçš„å‰ç¼€
            if any(keyword in text.lower() for keyword in ['ä¼˜ç‚¹', 'å¥½å¤„', 'ä¼˜åŠ¿']):
                prefix = "âœ…"
            elif any(keyword in text.lower() for keyword in ['ç¼ºç‚¹', 'é—®é¢˜', 'ä¸è¶³']):
                prefix = "âŒ"
            elif any(keyword in text.lower() for keyword in ['æ³¨æ„', 'è­¦å‘Š', 'å°å¿ƒ']):
                prefix = "âš ï¸"
            elif any(keyword in text.lower() for keyword in ['é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ']):
                prefix = "ğŸ”‘"
            else:
                prefix = "â€¢"
        
        return f"{prefix} {text}"
    
    def _extract_juejin(self, soup, url):
        """æå–æ˜é‡‘æ–‡ç« å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        soup = self._remove_unwanted_elements(soup)
        
        # æå–æ ‡é¢˜
        title_selectors = [
            'h1.article-title',
            '.article-title', 
            'h1',
            '[data-page-title]'
        ]
        
        title = "æœªçŸ¥æ ‡é¢˜"
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                break
        
        # æŸ¥æ‰¾æ–‡ç« å†…å®¹å®¹å™¨
        content_selectors = [
            '.markdown-body',
            '.article-content', 
            '[data-v-md-line]',
            '.content',
            'article',
            '.post-content'
        ]
        
        content_elem = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                break
        
        if not content_elem:
            # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|article|post'))
            if main_content:
                content_elem = main_content
        
        content = ""
        if content_elem:
            # è½¬æ¢ä¸ºmarkdownæ ¼å¼
            content = self._html_to_markdown_enhanced(content_elem)
        
        # æå–æ ‡ç­¾
        tags = self._extract_tags_juejin(soup)
        
        return title, content, tags
    
    def _extract_cnblogs(self, soup, url):
        """æå–åšå®¢å›­æ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='postTitle') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', id='cnblogs_post_body') or soup.find('div', class_='postBody')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        tags = ['æŠ€æœ¯åˆ†äº«', 'åšå®¢å›­']
        return title, content, tags
    
    def _extract_csdn(self, soup, url):
        """æå–CSDNæ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='title-article') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', id='content_views') or soup.find('div', class_='markdown_views')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        # æå–CSDNæ ‡ç­¾
        tag_elems = soup.find_all('a', class_='tag-link')
        tags = [tag.get_text().strip() for tag in tag_elems[:5]] if tag_elems else ['æŠ€æœ¯åˆ†äº«', 'CSDN']
        
        return title, content, tags
    
    def _extract_jianshu(self, soup, url):
        """æå–ç®€ä¹¦æ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', class_='show-content') or soup.find('article')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        tags = ['æŠ€æœ¯åˆ†äº«', 'ç®€ä¹¦']
        return title, content, tags
    
    def _extract_zhihu(self, soup, url):
        """æå–çŸ¥ä¹æ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='Post-Title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', class_='RichText') or soup.find('div', class_='Post-RichTextContainer')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        # æå–çŸ¥ä¹è¯é¢˜æ ‡ç­¾
        topic_elems = soup.find_all('a', class_='TopicLink')
        tags = [topic.get_text().strip() for topic in topic_elems[:5]] if topic_elems else ['çŸ¥è¯†åˆ†äº«', 'çŸ¥ä¹']
        
        return title, content, tags
    
    def _extract_segmentfault(self, soup, url):
        """æå–SegmentFaultæ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='article__title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', class_='article__content') or soup.find('div', class_='markdown-body')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        # æå–æ ‡ç­¾
        tag_elems = soup.find_all('a', class_='article-tag')
        tags = [tag.get_text().strip() for tag in tag_elems[:5]] if tag_elems else ['æŠ€æœ¯åˆ†äº«', 'SegmentFault']
        
        return title, content, tags
    
    def _extract_oschina(self, soup, url):
        """æå–å¼€æºä¸­å›½æ–‡ç« å†…å®¹"""
        soup = self._remove_unwanted_elements(soup)
        
        title_elem = soup.find('h1', class_='article-box__title') or soup.find('h1')
        title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        content_elem = soup.find('div', class_='article-detail') or soup.find('div', class_='content')
        content = self._html_to_markdown_enhanced(content_elem) if content_elem else ""
        
        tags = ['æŠ€æœ¯åˆ†äº«', 'å¼€æºä¸­å›½']
        return title, content, tags
    
    def _extract_tags_juejin(self, soup):
        """æå–æ˜é‡‘æ–‡ç« æ ‡ç­¾"""
        tag_selectors = [
            '.tag-list .tag',
            '.article-tag',
            '.tag',
            '[data-tag]'
        ]
        
        tags = []
        for selector in tag_selectors:
            tag_elems = soup.select(selector)
            if tag_elems:
                tags = [tag.get_text().strip() for tag in tag_elems[:6] if tag.get_text().strip()]
                break
        
        if not tags:
            tags = ['æŠ€æœ¯åˆ†äº«', 'æ˜é‡‘']
        
        return tags
    
    def _html_to_markdown_enhanced(self, element):
        """å¢å¼ºç‰ˆHTMLåˆ°Markdownè½¬æ¢"""
        if not element:
            return ""
        
        def process_element(elem, depth=0):
            """é€’å½’å¤„ç†HTMLå…ƒç´ """
            if isinstance(elem, NavigableString):
                text = str(elem).strip()
                return text if text else ""
            
            if not isinstance(elem, Tag):
                return ""
            
            tag_name = elem.name.lower()
            
            # è·³è¿‡ä¸éœ€è¦çš„æ ‡ç­¾
            if tag_name in ['script', 'style', 'meta', 'link', 'head']:
                return ""
            
            # è·å–å­å…ƒç´ å†…å®¹
            children_content = []
            for child in elem.children:
                child_result = process_element(child, depth + 1)
                if child_result:
                    children_content.append(child_result)
            
            text_content = ' '.join(children_content) if children_content else elem.get_text().strip()
            
            # æ ¹æ®æ ‡ç­¾ç±»å‹è¿›è¡Œè½¬æ¢
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag_name[1])
                if text_content:
                    # ä¸ºæ ‡é¢˜æ·»åŠ emoji
                    enhanced_title = self._enhance_title_with_emoji(text_content)
                    return f"\n\n{'#' * level} {enhanced_title}\n\n"
            
            elif tag_name == 'p':
                if text_content:
                    return f"\n\n{text_content}\n\n"
            
            elif tag_name in ['strong', 'b']:
                if text_content:
                    return f"**{text_content}**"
            
            elif tag_name in ['em', 'i']:
                if text_content:
                    return f"*{text_content}*"
            
            elif tag_name == 'code' and elem.parent.name != 'pre':
                if text_content:
                    return f"`{text_content}`"
            
            elif tag_name == 'pre':
                # å¤„ç†ä»£ç å—
                code_elem = elem.find('code')
                if code_elem:
                    return self._format_code_block(code_elem)
                else:
                    return f"\n\n```\n{text_content}\n```\n\n"
            
            elif tag_name in ['ul', 'ol']:
                # å¤„ç†åˆ—è¡¨
                list_items = []
                for i, li in enumerate(elem.find_all('li', recursive=False), 1):
                    li_content = process_element(li, depth + 1)
                    if li_content:
                        formatted_item = self._format_list_item(li, tag_name, i)
                        list_items.append(formatted_item)
                
                if list_items:
                    return f"\n\n" + "\n".join(list_items) + "\n\n"
            
            elif tag_name == 'li':
                return text_content
            
            elif tag_name == 'blockquote':
                if text_content:
                    # å¤„ç†å¼•ç”¨å—
                    quoted_lines = text_content.split('\n')
                    quoted_text = '\n'.join(f"> ğŸ’¡ {line.strip()}" for line in quoted_lines if line.strip())
                    return f"\n\n{quoted_text}\n\n"
            
            elif tag_name == 'a':
                href = elem.get('href', '')
                if text_content and href:
                    return f"[{text_content}]({href})"
                else:
                    return text_content
            
            elif tag_name == 'img':
                src = elem.get('src', '')
                alt = elem.get('alt', 'å›¾ç‰‡')
                if src:
                    return f"\n\n![{alt}]({src})\n\n"
            
            elif tag_name in ['table']:
                return f"\n\nğŸ“Š **æ•°æ®è¡¨æ ¼**\n\n{text_content}\n\n"
            
            elif tag_name in ['br']:
                return "\n"
            
            elif tag_name in ['hr']:
                return "\n\n---\n\n"
            
            elif tag_name in ['div', 'section', 'article']:
                # å¯¹äºå®¹å™¨å…ƒç´ ï¼Œè¿”å›å­å…ƒç´ å†…å®¹
                if children_content:
                    return ' '.join(children_content)
                else:
                    return text_content
            
            else:
                # å…¶ä»–æ ‡ç­¾ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹
                return text_content
        
        # å¤„ç†æ•´ä¸ªå…ƒç´ 
        result = process_element(element)
        
        # æ¸…ç†å’Œæ ¼å¼åŒ–ç»“æœ
        if result:
            result = self._clean_text(result)
            # ç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„åˆ†éš”
            result = re.sub(r'\n{3,}', '\n\n', result)
            # æ¸…ç†åˆ—è¡¨æ ¼å¼
            result = re.sub(r'\n+(â€¢|âœ…|âŒ|âš ï¸|ğŸ”‘|\d+\.)\s+', r'\n\1 ', result)
            
        return result
    
    def _extract_generic(self, soup, url):
        """é€šç”¨æ–‡ç« å†…å®¹æå–"""
        soup = self._remove_unwanted_elements(soup)
        
        # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = ['h1', '.title', '.article-title', '.post-title', '[data-title]']
        title = "æœªçŸ¥æ ‡é¢˜"
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                break
        
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            'article', '.content', '.article-content', '.post-content',
            '.markdown-body', '.article-body', '.entry-content',
            'main', '.main-content', '.post-body'
        ]
        content = ""
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = self._html_to_markdown_enhanced(content_elem)
                if len(content) > 100:  # ç¡®ä¿è·å–åˆ°è¶³å¤Ÿçš„å†…å®¹
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æ™ºèƒ½æå–
        if not content or len(content) < 100:
            content = self._smart_content_extraction(soup)
        
        tags = ['æ–‡ç« è½¬å‘', 'æŠ€æœ¯åˆ†äº«']
        return title, content, tags
    
    def _smart_content_extraction(self, soup):
        """æ™ºèƒ½å†…å®¹æå–ç®—æ³•"""
        # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½åŒ…å«å†…å®¹çš„div
        content_divs = soup.find_all('div')
        
        best_content = ""
        max_text_length = 0
        
        for div in content_divs:
            # è·³è¿‡æ˜æ˜¾çš„å¯¼èˆªã€å¹¿å‘Šç­‰åŒºåŸŸ
            div_class = ' '.join(div.get('class', [])).lower()
            div_id = (div.get('id') or '').lower()
            
            skip_patterns = ['nav', 'menu', 'sidebar', 'footer', 'header', 'ad', 'comment']
            if any(pattern in div_class or pattern in div_id for pattern in skip_patterns):
                continue
            
            div_text = div.get_text().strip()
            
            # é€‰æ‹©æ–‡æœ¬æœ€é•¿çš„divä½œä¸ºä¸»è¦å†…å®¹
            if len(div_text) > max_text_length:
                max_text_length = len(div_text)
                best_content = self._html_to_markdown_enhanced(div)
        
        return best_content
    
    async def fetch_article(self, url):
        """è·å–æ–‡ç« å†…å®¹"""
        print(f"ğŸŒ æ­£åœ¨è·å–æ–‡ç« : {url}")
        
        try:
            # å‘é€HTTPè¯·æ±‚
            response = requests.get(url, headers=self.headers, timeout=30)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None, None, None
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ ¹æ®ç½‘ç«™ç±»å‹é€‰æ‹©æå–å™¨
            domain = urlparse(url).netloc.lower()
            extractor = None
            
            for site, extract_func in self.site_configs.items():
                if site in domain:
                    extractor = extract_func
                    break
            
            if not extractor:
                extractor = self._extract_generic
            
            title, content, tags = extractor(soup, url)
            
            print(f"âœ… æ–‡ç« è·å–æˆåŠŸ:")
            print(f"ğŸ“ æ ‡é¢˜: {title}")
            print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"ğŸ·ï¸ æ ‡ç­¾: {tags}")
            
            return title, content, tags
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« å¤±è´¥: {e}")
            return None, None, None
    
    def _enhance_content_format(self, title, content, url, use_rich_text=True):
        """å¢å¼ºå†…å®¹æ ¼å¼"""
        enhanced_content = f"# {title}\n\n"
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        enhanced_content += f"> ğŸ“ **åŸæ–‡é“¾æ¥**: [{url}]({url})\n"
        enhanced_content += f"> â° **è½¬å‘æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
        enhanced_content += f"> ğŸŒŸ **å†…å®¹ä¼˜åŒ–**: å·²ä¼˜åŒ–æ’ç‰ˆæ ¼å¼ï¼Œæå‡é˜…è¯»ä½“éªŒ\n\n"
        enhanced_content += "---\n\n"
        
        # å¤„ç†å†…å®¹æ ¼å¼
        if content:
            # æ¸…ç†å†…å®¹
            content = self._clean_text(content)
            
            # ç¡®ä¿æ ‡é¢˜å±‚çº§æ­£ç¡®ï¼ˆé¿å…ä¸ä¸»æ ‡é¢˜å†²çªï¼‰
            content = re.sub(r'^# ', '## ', content, flags=re.MULTILINE)
            content = re.sub(r'^## ', '### ', content, flags=re.MULTILINE)
            content = re.sub(r'^### ', '#### ', content, flags=re.MULTILINE)
            
            # æ·»åŠ æ®µè½åˆ†éš”ç¬¦
            content = re.sub(r'\n\n([^#\n-*â€¢âœ…âŒâš ï¸ğŸ”‘>```])', r'\n\nğŸ“„ \1', content)
            
            enhanced_content += content
        else:
            enhanced_content += "æš‚æ— å†…å®¹æ‘˜è¦ï¼Œè¯·æŸ¥çœ‹åŸæ–‡é“¾æ¥ã€‚"
        
        # æ·»åŠ ç»“å°¾
        enhanced_content += "\n\n---\n\n"
        enhanced_content += "### ğŸ“ è½¬å‘è¯´æ˜\n\n"
        enhanced_content += "- ğŸ”— **åŸæ–‡é“¾æ¥**: è¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥æŸ¥çœ‹å®Œæ•´åŸæ–‡\n"
        enhanced_content += "- ğŸ“± **æ ¼å¼ä¼˜åŒ–**: å·²é’ˆå¯¹ç§»åŠ¨ç«¯é˜…è¯»ä½“éªŒè¿›è¡Œä¼˜åŒ–\n"
        enhanced_content += "- âš–ï¸ **ç‰ˆæƒå£°æ˜**: æœ¬æ–‡è½¬å‘è‡ªåŸä½œè€…ï¼Œå¦‚æœ‰ä¾µæƒè¯·è”ç³»åˆ é™¤\n\n"
        enhanced_content += "*æ„Ÿè°¢åŸä½œè€…çš„ç²¾å½©åˆ†äº«ï¼* ğŸ™"
        
        # å¦‚æœéœ€è¦å¯Œæ–‡æœ¬æ ¼å¼ï¼Œè½¬æ¢Markdownä¸ºçº¯æ–‡æœ¬æ ¼å¼
        if use_rich_text:
            print("ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºå¯Œæ–‡æœ¬æ ¼å¼...")
            rich_text_content = self._markdown_to_rich_text(enhanced_content)
            print(f"âœ… å¯Œæ–‡æœ¬è½¬æ¢å®Œæˆï¼Œé•¿åº¦: {len(rich_text_content)} å­—ç¬¦")
            return rich_text_content
        
        return enhanced_content
    
    def save_article_file(self, title, content, tags, url):
        """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
        # åˆ›å»ºæ–‡ä»¶åï¼ˆä½¿ç”¨URLçš„hashé¿å…é‡å¤ï¼‰
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        safe_title = re.sub(r'[^\w\s-]', '', title).strip()
        safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]
        
        filename = f"forwarded-enhanced-{safe_title}-{url_hash}.md"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        today = datetime.now().strftime('%Y-%m-%d')
        target_dir = f"articles/{today}"
        os.makedirs(target_dir, exist_ok=True)
        
        file_path = os.path.join(target_dir, filename)
        
        # å¢å¼ºå†…å®¹æ ¼å¼ï¼ˆä¿å­˜Markdownç‰ˆæœ¬ï¼‰
        enhanced_content = self._enhance_content_format(title, content, url, use_rich_text=False)
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(enhanced_content)
        
        print(f"ğŸ’¾ æ–‡ç« å·²ä¿å­˜: {file_path}")
        return file_path
    
    async def forward_to_toutiao(self, title, content, tags, url, account_file):
        """è½¬å‘æ–‡ç« åˆ°ä»Šæ—¥å¤´æ¡"""
        print("ğŸš€ å¼€å§‹å‘å¸ƒåˆ°ä»Šæ—¥å¤´æ¡...")
        
        try:
            # å¢å¼ºå†…å®¹æ ¼å¼å¹¶è½¬æ¢ä¸ºå¯Œæ–‡æœ¬
            enhanced_content = self._enhance_content_format(title, content, url, use_rich_text=True)
            
            # åˆ›å»ºæ–‡ç« å‘å¸ƒå¯¹è±¡
            article = TouTiaoArticle(
                title=title,
                content=enhanced_content,
                tags=tags,
                publish_date=0,  # ç«‹å³å‘å¸ƒ
                account_file=account_file,
                cover_path=None  # è‡ªåŠ¨ç”Ÿæˆå°é¢
            )
            
            # å‘å¸ƒæ–‡ç« 
            async with async_playwright() as playwright:
                await article.upload(playwright)
            
            print(f"âœ… æ–‡ç« å‘å¸ƒæˆåŠŸ: {title}")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡ç« å‘å¸ƒå¤±è´¥: {e}")
            return False

async def forward_article_from_url(url, account_file="cookies/toutiao_uploader/account.json", save_file=True):
    """ä»URLè½¬å‘æ–‡ç« åˆ°ä»Šæ—¥å¤´æ¡"""
    print("ğŸ”— å¢å¼ºç‰ˆæ–‡ç« é“¾æ¥è½¬å‘å·¥å…· v2.1")
    print("=" * 60)
    
    # æ£€æŸ¥ç™»å½•çŠ¶æ€
    print("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
    if not await toutiao_setup(account_file):
        print("âŒ ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆç™»å½•")
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤é‡æ–°ç™»å½•:")
        print("python examples/login_toutiao.py")
        return False
    
    print("âœ… ç™»å½•çŠ¶æ€æ­£å¸¸")
    
    # åˆ›å»ºè½¬å‘å™¨
    forwarder = EnhancedArticleForwarder()
    
    # è·å–æ–‡ç« å†…å®¹
    title, content, tags = await forwarder.fetch_article(url)
    
    if not title or not content:
        print("âŒ æ— æ³•è·å–æ–‡ç« å†…å®¹")
        return False
    
    # ä¿å­˜æ–‡ç« æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    if save_file:
        file_path = forwarder.save_article_file(title, content, tags, url)
    
    # ç¡®è®¤å‘å¸ƒ
    print(f"\nâš ï¸ å³å°†è½¬å‘æ–‡ç« åˆ°ä»Šæ—¥å¤´æ¡:")
    print(f"ğŸ“° æ ‡é¢˜: {title}")
    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"ğŸ·ï¸ æ ‡ç­¾: {tags}")
    print(f"ğŸ”— æ¥æº: {url}")
    print(f"ğŸ¨ æ’ç‰ˆ: å·²å¯ç”¨å¢å¼ºæ’ç‰ˆæ¨¡å¼")
    print(f"ğŸ”„ æ ¼å¼: Markdown â†’ å¯Œæ–‡æœ¬æ ¼å¼")
    
    confirm = input("\nç¡®è®¤è½¬å‘å—ï¼Ÿ(y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("âŒ ç”¨æˆ·å–æ¶ˆè½¬å‘")
        return False
    
    # è½¬å‘åˆ°ä»Šæ—¥å¤´æ¡
    success = await forwarder.forward_to_toutiao(title, content, tags, url, account_file)
    
    if success:
        print("\nğŸ‰ æ–‡ç« è½¬å‘å®Œæˆï¼")
        print("ğŸ“± è¯·ç™»å½•ä»Šæ—¥å¤´æ¡æŸ¥çœ‹å‘å¸ƒç»“æœ")
        print("âœ¨ æ’ç‰ˆå·²ä¼˜åŒ–ï¼Œé˜…è¯»ä½“éªŒæ›´ä½³")
        print("ğŸ”„ å†…å®¹å·²è½¬æ¢ä¸ºå¯Œæ–‡æœ¬æ ¼å¼ï¼Œæ— HTMLä»£ç ")
    else:
        print("\nâŒ æ–‡ç« è½¬å‘å¤±è´¥")
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆæ–‡ç« é“¾æ¥è½¬å‘åˆ°ä»Šæ—¥å¤´æ¡å·¥å…· v2.1')
    parser.add_argument('url', help='è¦è½¬å‘çš„æ–‡ç« é“¾æ¥')
    parser.add_argument('--account', default='cookies/toutiao_uploader/account.json', help='è´¦å·cookieæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜æ–‡ç« åˆ°æœ¬åœ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥URLæ ¼å¼
    if not args.url.startswith(('http://', 'https://')):
        print("âŒ è¯·æä¾›æœ‰æ•ˆçš„URLé“¾æ¥")
        return
    
    # æ£€æŸ¥è´¦å·æ–‡ä»¶
    if not os.path.exists(args.account):
        print(f"âŒ è´¦å·æ–‡ä»¶ä¸å­˜åœ¨: {args.account}")
        print("è¯·å…ˆè¿è¡Œç™»å½•è„šæœ¬: python examples/login_toutiao.py")
        return
    
    print(f"ğŸ”— ç›®æ ‡é“¾æ¥: {args.url}")
    print(f"ğŸ”‘ è´¦å·æ–‡ä»¶: {args.account}")
    print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {'å¦' if args.no_save else 'æ˜¯'}")
    print(f"âœ¨ ç‰ˆæœ¬: å¢å¼ºæ’ç‰ˆ v2.1 (å¯Œæ–‡æœ¬æ ¼å¼)")
    
    # è¿è¡Œè½¬å‘
    asyncio.run(forward_article_from_url(args.url, args.account, not args.no_save))

if __name__ == "__main__":
    main() 